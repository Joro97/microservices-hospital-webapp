{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Visualizations.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Tmguk4WrNjW-","colab_type":"code","colab":{}},"cell_type":"code","source":["# Mount code\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","from keras.models import load_model\n","import keras.metrics\n","import keras.preprocessing.image as image\n","import numpy as np\n","\n","def top_3_accuracy(y_true, y_pred):\n","    return keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)\n","\n","\n","def top_2_accuracy(y_true, y_pred):\n","    return keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=2)\n","\n","\n","keras.metrics.top_3_accuracy = top_3_accuracy\n","keras.metrics.top_2_accuracy = top_2_accuracy\n","\n","model = load_model('/content/gdrive/My Drive/Colab/model.h5') # change to your model.h5 file path"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xfnrkVItNrnE","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","from PIL import Image\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","generator = ImageDataGenerator(\n","    preprocessing_function=\n","    keras.applications.mobilenet.preprocess_input,\n","    rotation_range=180,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    fill_mode='nearest')\n","\n","generator_with_aug = generator.flow_from_directory(\n","    '/content/gdrive/My Drive/microservices-hospital-webapp/skin-cancer-model-training/divided_by_classes',\n","    target_size=(224, 224),\n","    batch_size=1) #There is only 1 picture in the directory\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_mxk9H5LO3eO","colab_type":"code","colab":{}},"cell_type":"code","source":["activations = model.predict_generator(generator_with_aug, steps=20)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D6T8tsuHPI1x","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib\n","from keras.models import Model\n","import matplotlib.pyplot as plt\n","\n","img_path = '/content/gdrive/My Drive/microservices-hospital-webapp/skin-cancer-model-training/divided_by_classes/mel/ISIC_0024310.jpg'\n","img = image.load_img(img_path, target_size=(224, 224))\n","img_tensor = image.img_to_array(img)\n","img_tensor = np.expand_dims(img_tensor, axis=0)\n","img_tensor /= 224.\n","plt.imshow(img_tensor[0])\n","plt.show()\n","print(img_tensor.shape)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9HE8VHc-TgOo","colab_type":"code","colab":{}},"cell_type":"code","source":["layer_outputs = [layer.output for layer in model.layers[1:]]\n","activation_model = Model(inputs=model.input, outputs=layer_outputs)\n","\n","activations = activation_model.predict(img_tensor)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2vzH31UlT2da","colab_type":"code","colab":{}},"cell_type":"code","source":["first_layer_activation = activations[0]\n","print(first_layer_activation.shape)\n","plt.matshow(first_layer_activation[0, :, :, 2], cmap='viridis')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KtCVJRr0UB7e","colab_type":"code","colab":{}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XcvJhPNNULoD","colab_type":"code","colab":{}},"cell_type":"code","source":["layer_names = []\n","\n","for layer in model.layers[1:-3]:\n","    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot \n","\n","images_per_row = 16\n","\n","\n","for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n","    n_features = layer_activation.shape[-1] # Number of features in the feature map\n","    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n","\n","    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n","    \n","    display_grid = np.zeros((size * n_cols, images_per_row * size))\n","    # print('heree')\n","    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n","        for row in range(images_per_row):\n","              channel_image = layer_activation[0,\n","                                               :, :,\n","                                               col * images_per_row + row]\n","              channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n","              channel_image /= channel_image.std()\n","              channel_image *= 64\n","              channel_image += 128\n","              channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n","              display_grid[col * size : (col + 1) * size, # Displays the grid\n","                         row * size : (row + 1) * size] = channel_image\n","             \n","            \n","    scale = 1. / size\n","    plt.figure(figsize=(scale * display_grid.shape[1],\n","                        scale * display_grid.shape[0]))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8H3WsP9CiREX","colab_type":"code","colab":{}},"cell_type":"code","source":["!git clone https://github.com/Joro97/microservices-hospital-webapp\n","  \n","# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once per notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","#\n","# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n","# Put the file id for the input zip from your google drive\n","# This is TsvetanKonstantinov input.zip file id. You will not be able to use it\n","file_id = '13d_Y4BYuIYIHVJvUE95c_hFTZZ0c8_tK'\n","downloaded = drive.CreateFile({'id': file_id})\n","print(downloaded)\n","\n","# The python scripts expect the input.zip to be in this directory\n","downloaded.GetContentFile(\"microservices-hospital-webapp/skin-cancer-model-training/input.zip\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-K-YV9Ci5Bw","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","files.upload()\n","\n","!mv definitions.py /content/microservices-hospital-webapp/skin-cancer-model-training/src/train/definitions.py"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fbeXuyCZi_5o","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","\n","ROOT_DIR = '/content/microservices-hospital-webapp/skin-cancer-model-training'\n","\n","INPUT_PATH = os.path.join(ROOT_DIR, 'For Drive')\n","\n","INPUT_ZIP_PATH = os.path.join(ROOT_DIR, 'input.zip')\n","VALIDATION_FOLDER_PATH = os.path.join(INPUT_PATH, 'validation')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mkYdK1m5kuq_","colab_type":"code","colab":{}},"cell_type":"code","source":["import zipfile\n","\n","with zipfile.ZipFile(INPUT_ZIP_PATH, 'r') as zip_ref:\n","    zip_ref.extractall(ROOT_DIR)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HFEtQbOtnRHX","colab_type":"code","colab":{}},"cell_type":"code","source":["LABELS = [\n","    'nv',\n","    'mel',\n","    'bkl',\n","    'bcc',\n","    'akiec',\n","    'vasc',\n","    'df'\n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cg594x8BkuzO","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_num_of_images(path):\n","    num = 0\n","\n","    for label in LABELS:\n","        imgs = os.listdir(os.path.join(path, label))\n","        num += len(imgs)\n","\n","    print('There are {} images in {}'.format(path, num))\n","\n","    return num"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pBe6g6dkku11","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","num_val_samples = get_num_of_images(VALIDATION_FOLDER_PATH)\n","\n","train_batch_size = 10\n","val_batch_size = 10\n","image_size = 224\n","\n","datagen = ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input)\n","\n","valid_batches = datagen.flow_from_directory(VALIDATION_FOLDER_PATH,\n","                                            target_size=(image_size,image_size),\n","                                            batch_size=val_batch_size)\n","\n","val_steps = np.ceil(num_val_samples / val_batch_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E85zIAJ2ku4j","colab_type":"code","colab":{}},"cell_type":"code","source":["model.metrics_names"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Zh6e9VgvFqt","colab_type":"code","colab":{}},"cell_type":"code","source":["val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = model.evaluate_generator(valid_batches,\n","                                                                               steps=val_steps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"64BhMy11v4id","colab_type":"code","colab":{}},"cell_type":"code","source":["print('val_loss:', val_loss)\n","print('val_cat_acc:', val_cat_acc)\n","print('val_top_2_acc:', val_top_2_acc)\n","print('val_top_3_acc:', val_top_3_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TBdGDOO1v_fl","colab_type":"code","colab":{}},"cell_type":"code","source":["val_labels = valid_batches.classes\n","print(valid_batches.class_indices)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uPumuwFtwkPU","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions = model.predict_generator(valid_batches,\n","                                      steps=val_steps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c65qHr2iwugB","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e88XY7dpw3Dy","colab_type":"code","colab":{}},"cell_type":"code","source":["val_labels.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6O1Go3skw5EN","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions.argmax(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y7XEFiAUUQRM","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ji1toHjPZmfG","colab_type":"code","colab":{}},"cell_type":"code","source":["cm = confusion_matrix(val_labels, predictions.argmax(axis=1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ErgZsrTCh50n","colab_type":"code","colab":{}},"cell_type":"code","source":["cm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n","plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sxOmMCvDxJxe","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the index of the class with the highest probability score\n","y_pred = np.argmax(predictions, axis=1)\n","\n","# Get the labels of the test images.\n","y_true = valid_batches.classes\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m99NINr_x9Rh","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Generate a classification report\n","report = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n","\n","print(report)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G0ERsaNryDVo","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}